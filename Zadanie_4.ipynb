{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.1, random_state=123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples):\n",
        "    probability = class_count / num_samples\n",
        "    return -(probability) * math.log(probability, 2)\n",
        "\n",
        "\n",
        "class Group:\n",
        "    def __init__(self, group_classes):\n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.group_classes.size\n",
        "\n",
        "    # assuming that group_classes is an array of classes\n",
        "    def group_entropy(self):\n",
        "        # determine classes of data ponits and calculate the entropy of the whole group\n",
        "        _, counts = np.unique(self.group_classes, return_counts=True)\n",
        "        entropy = np.sum([entropy_func(count, len(self)) for count in counts])\n",
        "        return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Node:\n",
        "    \"\"\"\n",
        "    split feature is the index of the feature that gives the best information gain\n",
        "    split_val is the value that gives the best information gain\n",
        "    depth is distance between the node and the root node\n",
        "    child_node_a is a node that collects the data points that meet split requirement\n",
        "    child_node_b is a node that collects the data points that do not meet split requirement\n",
        "    val is the predicted class for the data points that reach the node, if val is None, then the Node is not a leaf\n",
        "    if val is not None, then the Node is a leaf\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        split_feature,\n",
        "        split_val,\n",
        "        depth=None,\n",
        "        child_node_a=None,\n",
        "        child_node_b=None,\n",
        "        val=None,\n",
        "    ):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.depth = depth\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the predicted value recuresively, using children nodes\n",
        "    (if the node is a leaf return value, else pass the data point to a proper child node)\n",
        "    \"\"\"\n",
        "    def predict(self, data):\n",
        "        if not self.val == None:\n",
        "            return self.val\n",
        "        elif data[self.split_feature] < self.split_val:\n",
        "            return self.child_node_a.predict(data)\n",
        "        else:\n",
        "            return self.child_node_b.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier(object):\n",
        "    def __init__(self, max_depth):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the split entropy of two groups (a group is an array of possible classes)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a, group_b):\n",
        "        return (group_a.entropy * len(group_a) + group_b.entropy * len(group_b)) / (\n",
        "            len(group_a) + len(group_b)\n",
        "        )\n",
        "        \n",
        "    \"\"\"\n",
        "    Calculate the information gain of a given split\n",
        "    \"\"\"\n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        return parent_group.entropy - self.get_split_entropy(\n",
        "            child_group_a, child_group_b\n",
        "        )\n",
        "\n",
        "    \"\"\"\n",
        "    Find the best possible split for a given feature\n",
        "    \"\"\"\n",
        "    def get_best_feature_split(self, feature_values, classes):\n",
        "        best_information_gain = 0\n",
        "        best_feature_split = None\n",
        "        for value in feature_values:\n",
        "            indices_meeting_condition = [\n",
        "                i for i in range(len(classes)) if feature_values[i] < value\n",
        "            ]\n",
        "            indices_not_meeting_condition = [\n",
        "                i for i in range(len(classes)) if not i in indices_meeting_condition\n",
        "            ]\n",
        "            informtion_gain = self.get_information_gain(\n",
        "                Group(classes),\n",
        "                Group(classes[indices_meeting_condition]),\n",
        "                Group(classes[indices_not_meeting_condition]),\n",
        "            )\n",
        "            if informtion_gain > best_information_gain:\n",
        "                best_information_gain = informtion_gain\n",
        "                best_feature_split = value\n",
        "        return best_information_gain, best_feature_split\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Find the best possible split for the data set \n",
        "    (calculates all the possible splits for all of the possible features \n",
        "    returns the pair that gives the largest information gain)\n",
        "    \"\"\"\n",
        "    def get_best_split(self, data, classes):\n",
        "        best_information_gain = 0\n",
        "        best_feature = None\n",
        "        best_split = None\n",
        "        for i in range(len(data[0])):\n",
        "            information_gain, split_value = self.get_best_feature_split(\n",
        "                data[:, i], classes\n",
        "            )\n",
        "            if information_gain > best_information_gain:\n",
        "                best_information_gain = information_gain\n",
        "                best_feature = i\n",
        "                best_split = split_value\n",
        "        return best_feature, best_split\n",
        "\n",
        "    \"\"\"\n",
        "    Build a tree from given data recursevily, returns the root node.\n",
        "    \n",
        "    \"\"\"\n",
        "    def build_tree(self, data, classes, depth=0):\n",
        "        if len(np.unique(classes)) == 1: #Check if only one possible class in data\n",
        "            return Node(None, None, val=classes[0])\n",
        "        if all(all(element == data[0]) for element in data):\n",
        "            return Node(None, None, val=Counter(classes).most_common()[0][0]) #Check if furthur division makes sense (given data points are not all the same)\n",
        "        if depth == self.max_depth:\n",
        "            return Node(None, None, val=Counter(classes).most_common()[0][0]) #Check if depth limit reached\n",
        "        best_feature, best_split = self.get_best_split(data, classes) #Get the best possbile split\n",
        "        indices_meeting_condition = [\n",
        "            i for i in range(len(data)) if data[i][best_feature] < best_split\n",
        "        ]\n",
        "        indices_not_meeting_condition = [\n",
        "            i for i in range(len(data)) if not i in indices_meeting_condition\n",
        "        ]\n",
        "        child_a_value = None\n",
        "        child_b_value = None\n",
        "        if len(indices_meeting_condition) == 0: #Check if child_node_a will recieve any data points\n",
        "            child_a_value = Counter(classes).most_common()[0][0] #if not determine the value of child_node_a\n",
        "        if len(indices_not_meeting_condition) == 0: #Check if child_node_b will recieve any data points\n",
        "            child_b_value = Counter(classes).most_common()[0][0] #if not determine the value of child_node_B\n",
        "\n",
        "        #if child_node_a will not recieve data points, mark the child_node_a as a leaf, \n",
        "        #else calculate children nodes of child_node_a \n",
        "        child_node_a = (\n",
        "            Node(None, None, val=child_a_value)\n",
        "            if child_a_value\n",
        "            else self.build_tree(\n",
        "                data[indices_meeting_condition],\n",
        "                classes[indices_meeting_condition],\n",
        "                depth + 1,\n",
        "            )\n",
        "        )\n",
        "        #if child_node_b will not recieve data points, mark the child_node_b as a leaf, \n",
        "        #else calculate children nodes of child_node_b\n",
        "        child_node_b = (\n",
        "            Node(None, None, val=child_b_value)\n",
        "            if child_b_value\n",
        "            else self.build_tree(\n",
        "                data[indices_not_meeting_condition],\n",
        "                classes[indices_not_meeting_condition],\n",
        "                depth + 1,\n",
        "            )\n",
        "        )\n",
        "        return Node(best_feature, best_split, depth, child_node_a, child_node_b) #Return the root node\n",
        "\n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTING ID3:\n",
            "Test size: 0.1\n",
            "Random state = 123\n",
            "\n",
            "True values vs Predicred values:\n",
            "1. true value: 1; predicted value: 2\n",
            "2. true value: 2; predicted value: 2\n",
            "3. true value: 2; predicted value: 2\n",
            "4. true value: 1; predicted value: 1\n",
            "5. true value: 0; predicted value: 0\n",
            "6. true value: 2; predicted value: 2\n",
            "7. true value: 1; predicted value: 1\n",
            "8. true value: 0; predicted value: 0\n",
            "9. true value: 0; predicted value: 0\n",
            "10. true value: 1; predicted value: 1\n",
            "11. true value: 2; predicted value: 2\n",
            "12. true value: 0; predicted value: 0\n",
            "13. true value: 1; predicted value: 1\n",
            "14. true value: 2; predicted value: 2\n",
            "15. true value: 2; predicted value: 2\n",
            "Accuracy: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.1, random_state=123\n",
        ")\n",
        "dc = DecisionTreeClassifier(3)\n",
        "dc.tree = dc.build_tree(x_train, y_train)\n",
        "good_predictions = 0\n",
        "print(\"TESTING ID3:\")\n",
        "print(\"Test size: 0.1\")\n",
        "print(\"Random state = 123\\n\")\n",
        "print(\"True values vs Predicred values:\")\n",
        "counter = 1\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    print(f\"{counter}. true value: {gt}; predicted value: {prediction}\")\n",
        "    if prediction == gt:\n",
        "        good_predictions += 1\n",
        "    counter += 1\n",
        "print(f\"Accuracy: {good_predictions/len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(tree_depth, test_size, random_state=None, verbose=True):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "    dc = DecisionTreeClassifier(tree_depth)\n",
        "    dc.tree = dc.build_tree(x_train, y_train)\n",
        "    good_predictions = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        if prediction == gt:\n",
        "            good_predictions += 1\n",
        "    accuracy = good_predictions / len(y_test)\n",
        "    if verbose:\n",
        "        print(\"TESTING ID3:\")\n",
        "        print(f\"Tree depth {tree_depth}\")\n",
        "        print(f\"Test size: {test_size}\")\n",
        "        print(f\"Random state = {random_state}\")\n",
        "        print(f\"Accuracy = {accuracy}\\n\")\n",
        "    else:\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.1\n",
            "Random state = 123\n",
            "Accuracy = 0.9333333333333333\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.1\n",
            "Random state = 123\n",
            "Accuracy = 0.8666666666666667\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 1\n",
            "Test size: 0.1\n",
            "Random state = 123\n",
            "Accuracy = 0.6\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 5\n",
            "Test size: 0.1\n",
            "Random state = 123\n",
            "Accuracy = 0.9333333333333333\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 10\n",
            "Test size: 0.1\n",
            "Random state = 123\n",
            "Accuracy = 0.9333333333333333\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run_experiment(3, 0.1, 123)\n",
        "run_experiment(2, 0.1, 123)\n",
        "run_experiment(1, 0.1, 123)\n",
        "run_experiment(5, 0.1, 123)\n",
        "run_experiment(10, 0.1, 123)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.05\n",
            "Random state = 123\n",
            "Accuracy = 0.875\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.2\n",
            "Random state = 123\n",
            "Accuracy = 0.9666666666666667\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.3\n",
            "Random state = 123\n",
            "Accuracy = 0.9333333333333333\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.5\n",
            "Random state = 123\n",
            "Accuracy = 0.9733333333333334\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.8\n",
            "Random state = 123\n",
            "Accuracy = 0.8166666666666667\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.9\n",
            "Random state = 123\n",
            "Accuracy = 0.837037037037037\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.95\n",
            "Random state = 123\n",
            "Accuracy = 0.7622377622377622\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run_experiment(3, 0.05, 123)\n",
        "run_experiment(3, 0.2, 123)\n",
        "run_experiment(3, 0.3, 123)\n",
        "run_experiment(3, 0.5, 123)\n",
        "run_experiment(3, 0.8, 123)\n",
        "run_experiment(3, 0.9, 123)\n",
        "run_experiment(3, 0.95, 123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.05\n",
            "Random state = 123\n",
            "Accuracy = 0.75\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.2\n",
            "Random state = 123\n",
            "Accuracy = 0.9666666666666667\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.3\n",
            "Random state = 123\n",
            "Accuracy = 0.9555555555555556\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.5\n",
            "Random state = 123\n",
            "Accuracy = 0.96\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.8\n",
            "Random state = 123\n",
            "Accuracy = 0.8166666666666667\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.9\n",
            "Random state = 123\n",
            "Accuracy = 0.837037037037037\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 2\n",
            "Test size: 0.95\n",
            "Random state = 123\n",
            "Accuracy = 0.7622377622377622\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run_experiment(2, 0.05, 123)\n",
        "run_experiment(2, 0.2, 123)\n",
        "run_experiment(2, 0.3, 123)\n",
        "run_experiment(2, 0.5, 123)\n",
        "run_experiment(2, 0.8, 123)\n",
        "run_experiment(2, 0.9, 123)\n",
        "run_experiment(2, 0.95, 123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.1\n",
            "Random state = None\n",
            "Accuracy = 0.8666666666666667\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.1\n",
            "Random state = None\n",
            "Accuracy = 0.9333333333333333\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.1\n",
            "Random state = None\n",
            "Accuracy = 0.9333333333333333\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.1\n",
            "Random state = None\n",
            "Accuracy = 1.0\n",
            "\n",
            "TESTING ID3:\n",
            "Tree depth 3\n",
            "Test size: 0.1\n",
            "Random state = None\n",
            "Accuracy = 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "repetitions = 5\n",
        "for _ in range(repetitions):\n",
        "    run_experiment(3, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average depth three accuracy: 0.9479999999999997\n",
            "Average depth two accuracy: 0.9366666666666665\n",
            "Average depth one accuracy: 0.5986666666666669\n"
          ]
        }
      ],
      "source": [
        "repetitions = 100\n",
        "depth_three_accuracy = 0.0\n",
        "for _ in range(repetitions):\n",
        "    depth_three_accuracy += run_experiment(3, 0.1, verbose=False)\n",
        "\n",
        "depth_two_accuracy = 0.0\n",
        "for _ in range(repetitions):\n",
        "    depth_two_accuracy += run_experiment(2, 0.1, verbose=False)\n",
        "    \n",
        "depth_one_accuracy = 0.0\n",
        "for _ in range(repetitions):\n",
        "    depth_one_accuracy += run_experiment(1, 0.1, verbose=False)\n",
        "    \n",
        "print(f\"Average depth three accuracy: {depth_three_accuracy/repetitions}\")\n",
        "print(f\"Average depth two accuracy: {depth_two_accuracy/repetitions}\")\n",
        "print(f\"Average depth one accuracy: {depth_one_accuracy/repetitions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wnioski\n",
        "Drzewo decyzyjne ID3 osiąga dobrą skuteczność, jeśli dobrana jest głębokość odpowiednia do złożoności danych- tzn. im więcej wymiarów posiadają dane, tym większej głębokości drzewo będzie potrzebować, aby móć poprawnie przewidywać klasy. Zazwyczaj drzewo z większą głębokością osiągnie lepszą skuteczność przewidywań od drzewa z mniejszą głębokością, jednak jeśli maksymalna głębokość jest dostatecznie duża, jej zwiększanie nie przynosi żadnych rezultatów, gdyż drzewo przestaje być rozbudowywane ze względu na inne kryteria stopu. Rozmiar zbioru tesującego powinien być reprezentatywnym pozdbiorem dziedziny (wybrany zgodnie z rozkładem losowym i odpowiednio duży). W przeciwnym wypadku model nie będzie wytrenowany odpowiednio i nie będzie mógł skutecznie przewidywać klas przykładów."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
