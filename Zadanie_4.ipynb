{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.1, random_state=123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.array([1, 2, 3, 1, 1])\n",
        "c = Counter(a)\n",
        "c.most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples):\n",
        "    probability = class_count / num_samples\n",
        "    return -(probability) * math.log(probability, 2)\n",
        "\n",
        "\n",
        "class Group:\n",
        "    def __init__(self, group_classes):\n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.group_classes.size\n",
        "\n",
        "    # assuming that group_classes is an array of classes\n",
        "    def group_entropy(self):\n",
        "        # determine classes of data ponits\n",
        "        _, counts = np.unique(self.group_classes, return_counts=True)\n",
        "        entropy = np.sum([entropy_func(count, len(self)) for count in counts])\n",
        "        return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(\n",
        "        self,\n",
        "        split_feature,\n",
        "        split_val,\n",
        "        depth=None,\n",
        "        child_node_a=None,\n",
        "        child_node_b=None,\n",
        "        val=None,\n",
        "    ):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.depth = depth\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    def predict(self, data):\n",
        "        if not self.val == None:\n",
        "            return self.val\n",
        "        elif data[self.split_feature] < self.split_val:\n",
        "            return self.child_node_a.predict(data)\n",
        "        else:\n",
        "            return self.child_node_b.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 5]\n"
          ]
        }
      ],
      "source": [
        "a = np.array([1, 2, 3, 2, 1, 3])\n",
        "b = np.array([i for i in range(len(a)) if a[i] < 3])\n",
        "c = np.array([i for i in range(len(a)) if not i in b])\n",
        "\n",
        "\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier(object):\n",
        "    def __init__(self, max_depth):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a, group_b):\n",
        "        return (group_a.entropy * len(group_a) + group_b.entropy * len(group_b)) / (\n",
        "            len(group_a) + len(group_b)\n",
        "        )\n",
        "\n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        return parent_group.entropy - self.get_split_entropy(\n",
        "            child_group_a, child_group_b\n",
        "        )\n",
        "\n",
        "    def get_best_feature_split(self, feature_values, classes):\n",
        "        best_information_gain = 0\n",
        "        best_feature_split = None\n",
        "        for value in feature_values:\n",
        "            indices_meeting_condition = [\n",
        "                i for i in range(len(classes)) if feature_values[i] < value\n",
        "            ]\n",
        "            indices_not_meeting_condition = [\n",
        "                i for i in range(len(classes)) if not i in indices_meeting_condition\n",
        "            ]\n",
        "            informtion_gain = self.get_information_gain(\n",
        "                Group(classes),\n",
        "                Group(classes[indices_meeting_condition]),\n",
        "                Group(classes[indices_not_meeting_condition]),\n",
        "            )\n",
        "            if informtion_gain > best_information_gain:\n",
        "                best_information_gain = informtion_gain\n",
        "                best_feature_split = value\n",
        "        return best_information_gain, best_feature_split\n",
        "\n",
        "    def get_best_split(self, data, classes):\n",
        "        best_information_gain = 0\n",
        "        best_feature = None\n",
        "        best_split = None\n",
        "        for i in range(len(data[0])):\n",
        "            information_gain, split_value = self.get_best_feature_split(\n",
        "                data[:, i], classes\n",
        "            )\n",
        "            if information_gain > best_information_gain:\n",
        "                best_information_gain = information_gain\n",
        "                best_feature = i\n",
        "                best_split = split_value\n",
        "        return best_feature, best_split\n",
        "\n",
        "    def build_tree(self, data, classes, depth=0):\n",
        "        if len(np.unique(classes)) == 1:\n",
        "            return Node(None, None, val = classes[0])\n",
        "        if all(all(element == data[0]) for element in data):\n",
        "            return Node(None, None, val = Counter(classes).most_common()[0][0])\n",
        "        if depth == self.max_depth:\n",
        "            return Node(None, None, val = Counter(classes).most_common()[0][0])\n",
        "        best_feature, best_split = self.get_best_split(data, classes)\n",
        "        indices_meeting_condition = [\n",
        "            i for i in range(len(data)) if data[i][best_feature] < best_split\n",
        "        ]\n",
        "        indices_not_meeting_condition = [\n",
        "            i for i in range(len(data)) if not i in indices_meeting_condition\n",
        "        ]\n",
        "        child_a_value = None\n",
        "        child_b_value = None\n",
        "        if len(indices_meeting_condition) == 0:\n",
        "            child_a_value = Counter(classes).most_common()[0][0]\n",
        "        if len(indices_not_meeting_condition) == 0:\n",
        "            child_b_value = Counter(classes).most_common()[0][0]\n",
        "        if not child_a_value:\n",
        "            child_node_a = self.build_tree(data[indices_meeting_condition], classes[indices_meeting_condition], depth + 1)\n",
        "        else:\n",
        "            child_node_a = Node(None, None, val = child_a_value)\n",
        "        if not child_b_value:\n",
        "            child_node_b = self.build_tree(data[indices_not_meeting_condition], classes[indices_not_meeting_condition], depth + 1)\n",
        "        else:\n",
        "            child_node_b = Node(None, None, val= child_b_value)\n",
        "        return Node(best_feature, best_split, depth, child_node_a, child_node_b)\n",
        "        \n",
        "        \n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "U033RY1_YS8x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "dc = DecisionTreeClassifier(100)\n",
        "dc.tree = dc.build_tree(x_train, y_train)\n",
        "\n",
        "predictions = []\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    predictions.append(prediction)\n",
        "print(sum(predictions == y_test)/len(y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(all(all(element == d[0]) for element in d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
