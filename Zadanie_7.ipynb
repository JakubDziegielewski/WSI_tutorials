{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "# Zadanie 7 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie dwóch wersji naiwnego klasyfikatora Bayesa.\n",
        "* W pierwszej wersji należy dokonać dyskretyzacji danych - przedział wartości każdego atrybutu dzielimy na cztery równe przedziały i każdej ciągłej wartości atrybutu przypisujemy wartość dyskretną wynikająca z przynależności do danego przedziału.\n",
        "* W drugiej wersji wartości likelihood wyliczamy z rozkładów normalnych o średnich i odchyleniach standardowych wynikających z wartości atrybutów.\n",
        "\n",
        "Trening i test należy przeprowadzić dla zbioru Iris, tak jak w przypadku zadania z drzewem klasyfikacyjnym. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania klasyfikatorów dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Dyskretyzacja danych - **0.5 pkt**\n",
        "* Implementacja funkcji rozkładu normalnego o zadanej średniej i odchyleniu standardowym. - **0.5 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych dyskretnych. - **2.0 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych ciągłych. - **2.5 pkt**\n",
        "* Przeprowadzenie eksperymentów, wnioski i sposób ich prezentacji. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        self.priors = Counter(train_classes)\n",
        "        self.likelihoods = np.zeros(\n",
        "            shape=(len(self.priors), train_features.shape[1], 4)\n",
        "        )\n",
        "        for features, result_class in zip(train_features, train_classes):\n",
        "            for i, feature in enumerate(features):\n",
        "                self.likelihoods[result_class, i, feature] += 1\n",
        "        total = self.priors.total()\n",
        "        for key in self.priors.keys():\n",
        "            key_occurances = self.priors[key]\n",
        "            self.priors[key] /= total\n",
        "            for i in range(train_features.shape[1]):\n",
        "                for j in range(4):\n",
        "                    self.likelihoods[key, i, j] = (self.likelihoods[key, i, j] + 1) / (\n",
        "                        key_occurances + 4\n",
        "                    )\n",
        "\n",
        "    @staticmethod\n",
        "    def data_discretization(data, intervals):\n",
        "        discretize = (\n",
        "            lambda x: 0\n",
        "            if x < intervals[0]\n",
        "            else 1\n",
        "            if x < intervals[1]\n",
        "            else 2\n",
        "            if x < intervals[2]\n",
        "            else 3\n",
        "        )\n",
        "        return [discretize(x) for x in data]\n",
        "    \n",
        "    def find_intervals(self, train_data):\n",
        "        self.intervals = np.zeros(shape=(train_data.shape[1], 3))\n",
        "        for i, features in enumerate(train_data.T):\n",
        "            max_value = max(features)\n",
        "            min_value = min(features)\n",
        "            section_size = (max_value - min_value) / 4\n",
        "            first_section_limit = min_value + section_size\n",
        "            second_section_limit = first_section_limit + section_size\n",
        "            third_section_limit = second_section_limit + section_size\n",
        "            self.intervals[i] = np.array([first_section_limit, second_section_limit, third_section_limit])\n",
        "\n",
        "    def predict(self, sample):\n",
        "        max_probability = 0\n",
        "        prediction = None\n",
        "        for key in self.priors.keys():\n",
        "            probability = self.priors[key]\n",
        "            for i, feature in enumerate(sample):\n",
        "                probability *= self.likelihoods[key, i, feature]\n",
        "            if probability > max_probability:\n",
        "                prediction = key\n",
        "                max_probability = probability\n",
        "        return prediction\n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        self.priors = Counter(train_classes)\n",
        "        self.likelihoods = np.zeros(\n",
        "            shape=(len(self.priors), train_features.shape[1], 2)\n",
        "        )\n",
        "        total = self.priors.total()\n",
        "        for key in self.priors.keys():\n",
        "            self.priors[key] /= total\n",
        "            indices = np.where(train_classes == key)\n",
        "            for i, feature in enumerate(train_features[indices].T):\n",
        "                mean = np.mean(feature)\n",
        "                deviation = np.std(feature, ddof=1)\n",
        "                self.likelihoods[key, i, 0] = mean\n",
        "                self.likelihoods[key, i, 1] = deviation\n",
        "\n",
        "    @staticmethod\n",
        "    def normal_dist(x, mean, std):\n",
        "        return (\n",
        "            1\n",
        "            / (std * np.sqrt(2 * np.sqrt(2 * np.pi)))\n",
        "            * np.e ** (-0.5 * ((x - mean) / std) ** 2)\n",
        "        )\n",
        "\n",
        "    def predict(self, sample):\n",
        "        max_probability = 0\n",
        "        prediction = None\n",
        "        for key in self.priors.keys():\n",
        "            probability = self.priors[key]\n",
        "            for i, feature in enumerate(sample):\n",
        "                mean = self.likelihoods[key, i, 0]\n",
        "                deviation = self.likelihoods[key, i, 1]\n",
        "                probability *= GaussianNaiveBayes.normal_dist(feature, mean, deviation)\n",
        "            if probability > max_probability:\n",
        "                prediction = key\n",
        "                max_probability = probability\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: 1, True class: 1\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 1, True class: 2\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 2, True class: 2\n",
            "Accuracy: 0.933\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# x = np.array([NaiveBayes.data_discretization(r) for r in x.T]).T\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.1, random_state=123\n",
        ")\n",
        "\n",
        "nb = NaiveBayes()\n",
        "nb.find_intervals(x_train)\n",
        "intervals = nb.intervals\n",
        "x_train = np.array([NaiveBayes.data_discretization(features, intervals[i]) for i, features in enumerate(x_train.T)]).T\n",
        "x_test = np.array([NaiveBayes.data_discretization(features, intervals[i]) for i, features in enumerate(x_test.T)]).T\n",
        "\n",
        "good = 0\n",
        "total = 0\n",
        "nb = NaiveBayes()\n",
        "nb.build_classifier(x_train, y_train)\n",
        "for test_x, test_y in zip(x_test, y_test):\n",
        "    prediction = nb.predict(test_x)\n",
        "    print(f\"Prediction: {prediction}, True class: {test_y}\")\n",
        "    if prediction == test_y:\n",
        "        good += 1\n",
        "    total += 1\n",
        "print(f\"Accuracy: {good/total:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: 1, True class: 1\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 0, True class: 0\n",
            "Prediction: 1, True class: 1\n",
            "Prediction: 2, True class: 2\n",
            "Prediction: 2, True class: 2\n",
            "Accuracy: 1.000\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)\n",
        "good = 0\n",
        "total = 0\n",
        "nb = GaussianNaiveBayes()\n",
        "nb.build_classifier(x_train, y_train)\n",
        "for test_x, test_y in zip(x_test, y_test):\n",
        "    prediction = nb.predict(test_x)\n",
        "    print(f\"Prediction: {prediction}, True class: {test_y}\")\n",
        "    if prediction == test_y:\n",
        "        good += 1\n",
        "    total += 1\n",
        "print(f\"Accuracy: {good/total:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
