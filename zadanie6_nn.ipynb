{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 6\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski 1.[5 pkt]\n",
        "4. Jakość kodu 0.[5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "        self.X = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) ->np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size:int, output_size:int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.matrix(np.random.uniform(low=-1/np.sqrt(input_size), high=1/np.sqrt(input_size), size=(input_size, output_size)))\n",
        "        self.biases = np.random.randn(1, output_size)\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        self.X = x\n",
        "        return x @ self.weights + self.biases\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        input_error = output_error_derivative @ self.weights.T\n",
        "        weights_error = self.X.T @ output_error_derivative\n",
        "        self.weights -= self.learning_rate * weights_error\n",
        "        self.biases -= self.learning_rate * output_error_derivative\n",
        "        return input_error\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        self.X = x\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        print(self.X.shape)\n",
        "        return (1 - np.tanh(self.X)**2) * np.mean(output_error_derivative)\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x:np.ndarray, y:np.ndarray)->np.ndarray:\n",
        "        return self.loss_function(x, y)\n",
        "\n",
        "    def loss_derivative(self, x:np.ndarray, y:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        return self.loss_function_derivative(x, y)\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss:Loss)->None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        self.loss = loss.loss\n",
        "        self.loss_derivative = loss.loss_derivative\n",
        "\n",
        "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        layer_output = x\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer.forward(layer_output)\n",
        "        return layer_output\n",
        "\n",
        "    def fit(self,\n",
        "            x_train:np.ndarray,\n",
        "            y_train:np.ndarray,\n",
        "            epochs:int,\n",
        "            learning_rate:float,\n",
        "            verbose:int=0)->None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "        number_of_samples = len(x_train)\n",
        "        for i in range(epochs):\n",
        "            loss_value = 0\n",
        "            for j in range(number_of_samples):\n",
        "                output = self(x_train[j])\n",
        "                loss_value += self.loss(y_train[j], output)\n",
        "            error = self.loss_derivative(y_train[j], output)\n",
        "            for layer in reversed(self.layers):\n",
        "                error = layer.backward(error)\n",
        "            loss_value /= number_of_samples\n",
        "            if verbose > 0 and i % verbose == 0:\n",
        "                print(f\"epoch: {i}; loss_value={loss_value}\")\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 4])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([1,2]) ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 1)\n",
            "(1, 3)\n"
          ]
        },
        {
          "ename": "LinAlgError",
          "evalue": "Last 2 dimensions of the array must be square",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[99], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m net \u001b[38;5;241m=\u001b[39m Network([FullyConnected(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m), Tanh(), FullyConnected(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m), Tanh()], learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     12\u001b[0m net\u001b[38;5;241m.\u001b[39mcompile(loss)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[0;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m net(x_train)\n",
            "Cell \u001b[1;32mIn[96], line 103\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[1;34m(self, x_train, y_train, epochs, learning_rate, verbose)\u001b[0m\n\u001b[0;32m    101\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_derivative(y_train[j], output)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 103\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m loss_value \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m number_of_samples\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "Cell \u001b[1;32mIn[96], line 57\u001b[0m, in \u001b[0;36mTanh.backward\u001b[1;34m(self, output_error_derivative)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_error_derivative)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mnp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(output_error_derivative)\n",
            "File \u001b[1;32mc:\\Users\\JDzie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:232\u001b[0m, in \u001b[0;36mmatrix.__pow__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatrix_power\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\JDzie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\linalg.py:636\u001b[0m, in \u001b[0;36mmatrix_power\u001b[1;34m(a, n)\u001b[0m\n\u001b[0;32m    634\u001b[0m a \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[0;32m    635\u001b[0m _assert_stacked_2d(a)\n\u001b[1;32m--> 636\u001b[0m \u001b[43m_assert_stacked_square\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    639\u001b[0m     n \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(n)\n",
            "File \u001b[1;32mc:\\Users\\JDzie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\linalg.py:213\u001b[0m, in \u001b[0;36m_assert_stacked_square\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    211\u001b[0m m, n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m!=\u001b[39m n:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast 2 dimensions of the array must be square\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mLinAlgError\u001b[0m: Last 2 dimensions of the array must be square"
          ]
        }
      ],
      "source": [
        "def mse(x:np.ndarray, y:np.ndarray):\n",
        "    return np.power(y - x, 2)\n",
        "def mse_derivative(x: np.ndarray, y: np.ndarray):\n",
        "    return 2 * (y - x)/len(y)\n",
        "loss = Loss(mse, mse_derivative)\n",
        "\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "\n",
        "# network\n",
        "net = Network([FullyConnected(2, 3), Tanh(), FullyConnected(3, 1), Tanh()], learning_rate=0.1)\n",
        "net.compile(loss)\n",
        "\n",
        "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1, verbose=100)\n",
        "\n",
        "# test\n",
        "out = net(x_train)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.5"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.array([2, 3])\n",
        "b = np.array([1, 1])\n",
        "np.mean(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0.5910934 , 0.02975142, 0.76083433, 0.62899763],\n",
              "        [0.57939911, 0.11221394, 0.30080693, 0.63085744]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.matrix(np.random.random((2,3)))\n",
        "np.c_[a, np.random.random((2, 1))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
