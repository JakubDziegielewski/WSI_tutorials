{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 5\n",
        "\n",
        "Celem ćwiczenia jest implementacja algorytmu Q-learning.\n",
        "\n",
        "Następnie należy stworzyć agenta rozwiązującego problem [Taxi](https://gymnasium.farama.org/environments/toy_text/taxi/). Problem dostępny jest w pakiecie **gym**.\n",
        "\n",
        "Punktacja (max 7 pkt):\n",
        "- Implementacja algorytmu Q-learning. [3 pkt]\n",
        "- Eksperymenty dla różnych wartości hiperparametrów [2 pkt]\n",
        "- Jakość kodu [1 pkt]\n",
        "- Wnioski [1 pkt]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class QLearningSolver:\n",
        "    \"\"\"Class containing the Q-learning algorithm that might be used for different discrete environments.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: int,\n",
        "        action_space: int,\n",
        "        learning_rate: float = 0.9,\n",
        "        gamma: float = 0.9,\n",
        "        epsilon: float = 0.1,\n",
        "        q_table: np.ndarray = None,\n",
        "    ):\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        if q_table is None:\n",
        "            self.q_table = np.zeros(shape=(observation_space, action_space))\n",
        "        else:\n",
        "            self.q_table = q_table\n",
        "\n",
        "    def __call__(self, state: np.ndarray, action: np.ndarray) -> float:\n",
        "        \"\"\"Return Q-value of given state and action.\"\"\"\n",
        "        return self.q_table[state][action]\n",
        "\n",
        "    def update(self, state: np.ndarray, action: np.ndarray, reward: float) -> None:\n",
        "        \"\"\"Update Q-value of given state and action.\"\"\"\n",
        "        self.q_table[state][action] += reward\n",
        "\n",
        "    def get_best_action(self, state: np.ndarray) -> int:\n",
        "        \"\"\"Return action that maximizes Q-value for a given state.\"\"\"\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def get_best_move_evaluation(self, state: np.array) -> float:\n",
        "        return np.max(self.q_table[state])\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Elegant representation of Q-learning solver.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_episode(solver: QLearningSolver, environment):\n",
        "    state = environment.reset()[0]\n",
        "    terminated, truncated = False, False\n",
        "\n",
        "    while not terminated and not truncated:\n",
        "        if np.random.random() < solver.epsilon:\n",
        "            action = environment.action_space.sample()\n",
        "        else:\n",
        "            action = solver.get_best_action(state)\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = environment.step(action)\n",
        "        delta = (\n",
        "            reward\n",
        "            + solver.gamma * solver.get_best_move_evaluation(next_state)\n",
        "            - solver(state, action)\n",
        "        )\n",
        "        solver.update(state, action, solver.learning_rate * delta)\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "def q_learning(\n",
        "    environment, learning_rate=0.9, gamma=0.9, epsilon=0.1, number_of_episodes=1000\n",
        "):\n",
        "    solver = QLearningSolver(\n",
        "        environment.observation_space.n,\n",
        "        environment.action_space.n,\n",
        "        learning_rate,\n",
        "        gamma,\n",
        "        epsilon,\n",
        "    )\n",
        "    for _ in range(number_of_episodes):\n",
        "        run_episode(solver, environment)\n",
        "    return solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_solver(solver: QLearningSolver, environment, number_of_tests: int = 100):\n",
        "    successes = 0\n",
        "    total_steps = 0\n",
        "    for _ in range(number_of_tests):\n",
        "        state = environment.reset()[0]\n",
        "        terminated, truncated = False, False\n",
        "        steps = 0\n",
        "        while not terminated and not truncated:\n",
        "            action = solver.get_best_action(state)\n",
        "            next_state, reward, terminated, truncated = environment.step(action)[:4]\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        if terminated and reward > 0:\n",
        "            successes += 1\n",
        "        total_steps += steps\n",
        "    return successes / number_of_tests, total_steps / number_of_tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(\n",
        "    env, learning_rate=0.9, gamma=0.9, epsilon=0.1, number_of_episodes=1000\n",
        "):\n",
        "    solver = q_learning(env, learning_rate, gamma, epsilon, number_of_episodes)\n",
        "    success_rate, average_number_of_steps = test_solver(solver, env)\n",
        "    print(\"LEARNING:\")\n",
        "    print(\n",
        "        f\"Learning rate: {learning_rate}, gamma: {gamma}, epsilon: {epsilon}, number of episodes: {number_of_episodes}\"\n",
        "    )\n",
        "    print(\"TESTING:\")\n",
        "    print(\n",
        "        f\"Success Rate: {success_rate}\\nAverage number of steps: {average_number_of_steps}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.9, epsilon: 0.1, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.98\n",
            "Averagenumber of steps: 16.69\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.9, epsilon: 0.1, number of episodes: 800\n",
            "TESTING:\n",
            "Success Rate: 0.94\n",
            "Averagenumber of steps: 24.62\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, number_of_episodes=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.9, epsilon: 0.1, number of episodes: 600\n",
            "TESTING:\n",
            "Success Rate: 0.9\n",
            "Averagenumber of steps: 32.11\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, number_of_episodes=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.9, epsilon: 0.1, number of episodes: 1500\n",
            "TESTING:\n",
            "Success Rate: 1.0\n",
            "Average number of steps: 13.01\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, number_of_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.7, epsilon: 0.1, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.97\n",
            "Average number of steps: 18.46\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, gamma = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.7, epsilon: 0.1, number of episodes: 1500\n",
            "TESTING:\n",
            "Success Rate: 1.0\n",
            "Average number of steps: 13.17\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, gamma = 0.7, number_of_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.6, gamma: 0.9, epsilon: 0.1, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.98\n",
            "Average number of steps: 17.0\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.8, gamma: 0.9, epsilon: 0.5, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 1.0\n",
            "Averagenumber of steps: 12.99\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.8, epsilon=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.5, gamma: 0.9, epsilon: 0.5, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 1.0\n",
            "Averagenumber of steps: 12.96\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.5, epsilon=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.1, gamma: 0.9, epsilon: 0.5, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.44\n",
            "Averagenumber of steps: 116.77\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.1, epsilon=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success ratio: 0.853\n",
            "Average number of steps: 18.173\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.9, epsilon=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.9, epsilon: 0.25, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.99\n",
            "Average number of steps: 15.41\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.9, gamma=0.9, epsilon=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.2, gamma: 0.9, epsilon: 0.4, number of episodes: 2500\n",
            "TESTING:\n",
            "Success Rate: 1.0\n",
            "Average number of steps: 12.94\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.2, gamma=0.9, epsilon=0.4, number_of_episodes=2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.2, gamma: 0.9, epsilon: 0.4, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.72\n",
            "Average number of steps: 65.29\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.2, gamma=0.9, epsilon=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.8, gamma: 0.4, epsilon: 0.4, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.9\n",
            "Average number of steps: 31.9\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.8, gamma=0.4, epsilon=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.8, gamma: 0.4, epsilon: 0.7, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.99\n",
            "Average number of steps: 15.16\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.8, gamma=0.4, epsilon=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.9, gamma: 0.4, epsilon: 0.4, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.96\n",
            "Average number of steps: 20.88\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.9, gamma=0.4, epsilon=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.99, gamma: 0.8, epsilon: 0.2, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.95\n",
            "Average number of steps: 22.16\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.99, gamma=0.8, epsilon=0.2, number_of_episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LEARNING:\n",
            "Learning rate: 0.7, gamma: 0.8, epsilon: 0.2, number of episodes: 1000\n",
            "TESTING:\n",
            "Success Rate: 0.96\n",
            "Average number of steps: 20.57\n"
          ]
        }
      ],
      "source": [
        "run_experiment(env, learning_rate=0.7, gamma=0.8, epsilon=0.2, number_of_episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "solver = q_learning(\n",
        "    env, learning_rate=0.9, gamma=0.9, epsilon=0.4, number_of_episodes=1000\n",
        ")\n",
        "np.save(\"solver\", solver.q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Skuteczność algorytmu QLearning w dużym stopniu zależy od poziomu skomplikowania środowisk i dopowiedniego doboru hiperparametrów. \n",
        "\n",
        "Jeśli epsilon jest zbyt mały, przestrzeń nie będzie eksplorowana w odpowiednim stopniu, jednak jeśli epsilon będzie zbyt duży, zdobyta wiedza będzie wykorzystywana w małym stopniu, a co za tym idzie algorytm może nie dojść do stanu akceptującego w rozsądnym czasie.\n",
        "\n",
        "Zbyt duży learning rate powoduje, że to czego algorytm nauczy się w początkowych epizodach może zostać zapomniane w trakcie późniejszych epizodów, co poskutkuje obniżeniem skuteczności. Jednak mniejszy learning rate, sprawia że potrzeba większej liczby epizodów trenujących do uzyskania satysfakcjonujących wyników. \n",
        "\n",
        "Gamma jest odpowiedzialna za szybkość dążenia do potencjalnych nagród. Duża wartość parametru, wskazuje że preferowane są większe nagrody, nawet jeśli do ich uzysania należy poświęcić więcej wysiłku (preferencja nagród długoterminowych). Zbyt duża wartość gammy prowadzi do małej eksploatacji, natomiast zbyt mała gamma prowadzi do za małej eksploracji.\n",
        "\n",
        "W skomplikowanych środowiskach pomocne okazać się może zmniejszanie hiperparametrów epsilon i learning rate wraz ze wzrostem wiedzy na temat środowiska, a także zmniejszanie parametru gamma w późniejszych fazach każdego epizodu. Zmniejszanie epsilonu pozwala na wykorzystanie zdobytej wiedzy, dzięki czemu algorytm nie musi się uczyć kilka razy tych samych ścieżek. Dzięki zmniejszeniu learning rate algorytm nie zapomina informacji, które zdobył w trakcie wcześniejszych epizodów. Stopniowe zmniejszanie parametru gamma w każdym epizodzie może pozwolić na preferowanie szybkich nagród, w późniejszych fragmentach epizodu, czyli kiedy algorytm nie ma dużo czasu na próby zdobycia nagród długoterminowych."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
