{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 6\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski 1.[5 pkt]\n",
        "4. Jakość kodu 0.[5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert (\n",
        "            learning_rate < 1\n",
        "        ), f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert (\n",
        "            learning_rate > 0\n",
        "        ), f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size: int, output_size: int, random_seed: int = 0) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        np.random.seed(random_seed)\n",
        "        self.weights = np.random.uniform(\n",
        "            low=-1 / np.sqrt(input_size),\n",
        "            high=1 / np.sqrt(input_size),\n",
        "            size=(input_size, output_size),\n",
        "        )\n",
        "        self.biases = np.random.randn(1, output_size)\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.X = x\n",
        "        self.Y = self.X @ self.weights + self.biases\n",
        "        return self.Y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        input_error = output_error_derivative @ self.weights.T\n",
        "        weights_error = self.X.T @ output_error_derivative\n",
        "\n",
        "        self.weights -= self.learning_rate * weights_error\n",
        "        self.biases -= self.learning_rate * output_error_derivative\n",
        "        return input_error\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.X = x\n",
        "        self.Y = np.tanh(x)\n",
        "        return self.Y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        return (1 - np.tanh(self.X) ** 2) * output_error_derivative\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    def __init__(\n",
        "        self, loss_function: callable, loss_function_derivative: callable\n",
        "    ) -> None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        return self.loss_function(x, y)\n",
        "\n",
        "    def loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        return self.loss_function_derivative(x, y)\n",
        "\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers: List[Layer], learning_rate: float = 0.01) -> None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        if learning_rate != 0.01:\n",
        "            for layer in self.layers:\n",
        "                layer.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss: Loss) -> None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        self.loss = loss\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        x_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        epochs: int,\n",
        "        learning_rate: float = 0.01,\n",
        "        verbose: int = 0,\n",
        "    ) -> None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "        number_of_samples = len(x_train)\n",
        "        if learning_rate != self.learning_rate:\n",
        "            for layer in self.layers:\n",
        "                layer.learning_rate = learning_rate\n",
        "        for i in range(epochs):\n",
        "            error = 0\n",
        "            for j in range(number_of_samples):\n",
        "                output = self(x_train[j])\n",
        "                error += np.mean(self.loss.loss(output, y_train[j]))\n",
        "                error_derivative = self.loss.loss_derivative(output, y_train[j])\n",
        "                for layer in reversed(self.layers):\n",
        "                    error_derivative = layer.backward(error_derivative)\n",
        "            if verbose > 0 and (i + 1) % verbose == 0:\n",
        "                error /= number_of_samples\n",
        "                print(f\"episode number: {i+1} error on training set={error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode number: 100 error on training set=0.36749530913651973\n",
            "episode number: 200 error on training set=0.0003332883517798109\n",
            "episode number: 300 error on training set=1.200299421915983e-06\n",
            "episode number: 400 error on training set=1.7892369635903826e-05\n",
            "episode number: 500 error on training set=2.5733460693620443e-11\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[[0, 0]], [[0, 1]], [[1, 0]], [[1, 1]]])\n",
        "y = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "network = Network([FullyConnected(2, 5), Tanh(), FullyConnected(5,3), Tanh(), FullyConnected(3, 1)])\n",
        "loss = Loss(lambda x, y: np.power(y - x, 2), lambda x, y: 2 * (x - y) / y.size)\n",
        "network.compile(loss)\n",
        "network.fit(x, y, 500, verbose=100, learning_rate=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_layers(dimensions_list, random_seed = 0):\n",
        "    layers = []\n",
        "    for dimensions in dimensions_list:\n",
        "        layers.append(FullyConnected(dimensions[0], dimensions[1], random_seed))\n",
        "        layers.append(Tanh())\n",
        "    return layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(\n",
        "    dimensions_list, number_of_epochs, learning=0.01, test_ratio=0.1, random_seed=0, verbose = 0\n",
        "):\n",
        "    layers = init_layers(dimensions_list, random_seed)\n",
        "    msint = load_digits()\n",
        "    x = np.array(msint.data)\n",
        "    y = np.array(msint.target)\n",
        "\n",
        "    y_temp = np.zeros((len(y), 10))\n",
        "    for array, value in zip(y_temp, y):\n",
        "        array[value] = 1\n",
        "    y = y_temp\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x, y, test_size=test_ratio, random_state=random_seed\n",
        "    )\n",
        "\n",
        "    network = Network(layers)\n",
        "\n",
        "    loss = Loss(lambda x, y: np.power(y - x, 2), lambda x, y: 2 * (x - y) / y.size)\n",
        "\n",
        "    network.compile(loss)\n",
        "    network.fit(\n",
        "        np.array(x_train).reshape((-1, 1, 64)),\n",
        "        y_train,\n",
        "        epochs=number_of_epochs,\n",
        "        learning_rate=learning,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    error = 0\n",
        "    error_with_rounding = 0\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        out = network(x)\n",
        "        rounded = np.round(out)\n",
        "        error += (y - out) ** 2\n",
        "        error_with_rounding += (y - rounded) ** 2\n",
        "    return(sum(error[0]) / len(y_test), sum(error_with_rounding[0]) / len(y_test))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error of test set: 0.1907\n",
            "Error of test set with rounding final output vector: 0.1056\n"
          ]
        }
      ],
      "source": [
        "error, error_with_rounding = run_experiment([(64, 80), (80, 60), (60, 30), (30, 10)], 10, learning=0.01, verbose=100)\n",
        "print(f\"Error of test set: {error:.4f}\\nError of test set with rounding final output vector: {error_with_rounding:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0., 0.])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
