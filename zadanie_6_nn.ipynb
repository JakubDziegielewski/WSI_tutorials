{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 6\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski 1.[5 pkt]\n",
        "4. Jakość kodu 0.[5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert (\n",
        "            learning_rate < 1\n",
        "        ), f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert (\n",
        "            learning_rate > 0\n",
        "        ), f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size: int, output_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.uniform(\n",
        "            low=-1 / np.sqrt(input_size),\n",
        "            high=1 / np.sqrt(input_size),\n",
        "            size=(input_size, output_size),\n",
        "        )\n",
        "        self.biases = np.random.randn(1, output_size)\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.X = x\n",
        "        self.Y = self.X @ self.weights + self.biases\n",
        "        return self.Y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        input_error = output_error_derivative @ self.weights.T\n",
        "        weights_error = self.X.T @ output_error_derivative\n",
        "        # dBias = output_error\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= self.learning_rate * weights_error\n",
        "        self.biases -= self.learning_rate * output_error_derivative\n",
        "        return input_error\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.X = x\n",
        "        self.Y = np.tanh(x)\n",
        "        return self.Y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        return (1 - np.tanh(self.X) ** 2) * output_error_derivative\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    def __init__(\n",
        "        self, loss_function: callable, loss_function_derivative: callable\n",
        "    ) -> None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        return self.loss_function(x, y)\n",
        "\n",
        "    def loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        return self.loss_function_derivative(x, y)\n",
        "\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers: List[Layer], learning_rate: float = 0.01) -> None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        if learning_rate != 0.01:\n",
        "            for layer in self.layers:\n",
        "                layer.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss: Loss) -> None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        self.loss = loss\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        x_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        epochs: int,\n",
        "        learning_rate: float = 0.01,\n",
        "        verbose: int = 0,\n",
        "    ) -> None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "        number_of_samples = len(x_train)\n",
        "        if learning_rate != self.learning_rate:\n",
        "            for layer in self.layers:\n",
        "                layer.learning_rate = learning_rate\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(number_of_samples):\n",
        "                output = self(x_train[j])\n",
        "                err += np.mean(self.loss.loss(output, y_train[j]))\n",
        "                error = self.loss.loss_derivative(output, y_train[j])\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward(error)\n",
        "            if verbose > 0 and (i + 1) % verbose == 0:\n",
        "                err /= number_of_samples\n",
        "                print(f\"episode number: {i+1}   error={err}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode number: 100   error=0.23723424308807692\n",
            "episode number: 200   error=0.009953493899367387\n",
            "episode number: 300   error=0.0018409468986533004\n",
            "episode number: 400   error=0.0009443000579354027\n",
            "episode number: 500   error=0.0006222579087431951\n",
            "episode number: 600   error=0.0004596501074596519\n",
            "episode number: 700   error=0.00036246412832132243\n",
            "episode number: 800   error=0.00029817412288108666\n",
            "episode number: 900   error=0.0002526537730309262\n",
            "episode number: 1000   error=0.00021881263756939147\n",
            "[[0.00092376]]\n",
            "[[0.97917778]]\n",
            "[[0.97906637]]\n",
            "[[-0.00038621]]\n"
          ]
        }
      ],
      "source": [
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "\n",
        "def mse(x:np.ndarray, y:np.ndarray):\n",
        "    return np.power(y - x, 2)\n",
        "    \n",
        "def mse_derivative(x:np.ndarray, y:np.ndarray):\n",
        "    return 2 * (x - y) / y.size\n",
        "# network\n",
        "net = Network([FullyConnected(2, 3), Tanh(), FullyConnected(3, 1), Tanh()], learning_rate=0.01)\n",
        "\n",
        "\n",
        "# train\n",
        "net.compile(Loss(mse, mse_derivative))\n",
        "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1, verbose=100)\n",
        "\n",
        "# test\n",
        "for x in x_train:\n",
        "    out = net(x)\n",
        "    print(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.07193758565961478\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def mse(x:np.ndarray, y:np.ndarray):\n",
        "    return np.power(y - x, 2)\n",
        "    \n",
        "def mse_derivative(x:np.ndarray, y:np.ndarray):\n",
        "    return 2 * (x - y) / y.size\n",
        "\n",
        "msint = load_digits()\n",
        "x = np.array(msint.data)\n",
        "y = np.array(msint.target)\n",
        "\n",
        "y_temp = np.zeros((len(y), 10))\n",
        "for array, value in zip(y_temp, y):\n",
        "    array[value] = 1\n",
        "y = y_temp\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.1, random_state=123\n",
        ")\n",
        "\n",
        "net = Network([FullyConnected(64, 50), Tanh(), FullyConnected(50, 30), Tanh(), FullyConnected(30, 10)], learning_rate=0.1)\n",
        "\n",
        "net.compile(Loss(mse, mse_derivative))\n",
        "net.fit(np.array(x_train).reshape((x_train.shape[0], 1, 64)), y_train, epochs=300, learning_rate=0.1)\n",
        "\n",
        "error = 0\n",
        "for x, y in zip(x_test, y_test):\n",
        "    out = net(x)\n",
        "    error += (y - out)**2\n",
        "print(sum(error[0])/len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "msint = load_digits()\n",
        "x = np.array(msint.data)\n",
        "y = msint.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.1, random_state=123\n",
        ")\n",
        "y_train[70]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
